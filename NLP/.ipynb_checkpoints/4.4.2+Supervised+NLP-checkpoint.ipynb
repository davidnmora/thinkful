{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Supervised NLP requires a pre-labelled dataset for training and testing, and is generally interested in categorizing text in various ways. In this case, we are going to try to predict whether a sentence comes from _Alice in Wonderland_ by Lewis Carroll or _Persuasion_ by Jane Austen. We can use any of the supervised models we've covered previously, as long as they allow categorical outcomes. In this case, we'll try Random Forests, SVM, and KNN.\n",
    "\n",
    "Our feature-generation approach will be something called _BoW_, or _Bag of Words_. BoW is quite simple: For each sentence, we count how many times each word appears. We will then use those counts as features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y, pred_y):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y, pred_y)\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm[:].sum() - (FP + FN + TP)\n",
    "\n",
    "    TPR = TP/(TP+FN)\n",
    "    print('Sensitivity, hit rate, recall, or true positive rate: ', TPR[1])\n",
    "    TNR = TN/(TN+FP) \n",
    "    print('Specificity or true negative rate: ', TNR[1])\n",
    "    PPV = TP/(TP+FP)\n",
    "    print('Precision or positive predictive value: ', PPV[1])\n",
    "    NPV = TN/(TN+FN)\n",
    "    print('Negative predictive value: ', NPV[1])\n",
    "    FPR = FP/(FP+TN)\n",
    "    print('Fall out or false positive rate: ', FPR[1])\n",
    "    FNR = FN/(TP+FN)\n",
    "    print('False negative rate: ', FNR[1])\n",
    "    FDR = FP/(TP+FP)\n",
    "    print('False discovery rate: ', FDR)\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "    print('Overall accuracy: ', ACC)\n",
    "\n",
    "    \n",
    "def print_confusion_matrix(trained_model, X, y):\n",
    "    pred_y = trained_model.predict(X)\n",
    "    print('\\nConfusion matrix for test set:')\n",
    "    print(pd.crosstab(pred_y, y))\n",
    "    \n",
    "    print(perf_measure(y, pred_y))\n",
    "\n",
    "    \n",
    "def run_train_test_split(model_instance, X, y, pretrained_model=False):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y,\n",
    "                                                        test_size=0.4,\n",
    "                                                        random_state=0)\n",
    "    if not pretrained_model:\n",
    "        model_instance.fit(X_train, y_train)\n",
    "    print('Training set score:', model_instance.score(X_train, y_train))\n",
    "    print('\\nTest set score:', model_instance.score(X_test, y_test))\n",
    "    print_confusion_matrix(model_instance, X_test, y_test)\n",
    "\n",
    "    \n",
    "def run_model(model_instance, X=None, y=None, df=None, outcome_col=None, folds=5, pretrained_model=False):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    X = X if X.any() else df.drop(outcome_col, axis='columns')\n",
    "    y = y if y.any() else df[outcome_col]\n",
    "    \n",
    "    top_outcome = y.describe().top\n",
    "    percent_of_top_outcome = round(100 * y.describe().freq / len(y), 2)\n",
    "    print(f'Class balance: {top_outcome} comprises {percent_of_top_outcome}% of dataset.')\n",
    "    \n",
    "    if not pretrained_model:  # we can run cross-validation (which trains the model)\n",
    "        cv_scores = cross_val_score(model_instance, X, y, cv=folds)\n",
    "        print(f'\\nCross Val ({folds} folds):')\n",
    "        print(f'Avg: {round(100*cv_scores.mean(), 2)} \\nStd: {round(100*cv_scores.std(), 2)}')\n",
    "        \n",
    "    run_train_test_split(model_instance, X, y, pretrained_model=pretrained_model)\n",
    "    print('__DONE__')\n",
    "    return model_instance, X, y\n",
    "\n",
    "\n",
    "def get_feature_importance(trained_model, X):\n",
    "    importances = trained_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    names = [X.columns[i] for i in indices]\n",
    "    \n",
    "    # Plot the feature importances of the trained_model\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(X.shape[1]), importances[indices],\n",
    "           color=\"r\", align=\"center\")\n",
    "    plt.xticks(range(X.shape[1]), names)\n",
    "    plt.xlim([-1, X.shape[1]])\n",
    "    for tick in plt.gca().get_xticklabels():\n",
    "        tick.set_rotation(85)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Load text, clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: Can TO 10K CHARS FOR DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHARACTERS = int(1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def load_and_clean_text(name, removal_regex):\n",
    "    text = gutenberg.raw(name)\n",
    "    text = re.sub(removal_regex, '', text)\n",
    "    return text_cleaner(text)[0:MAX_CHARACTERS]  # Too big text breaks spaCy with a warming/error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Load via spacy, prase into sentences (used in df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_into_doc_and_sents(text, label):\n",
    "    '''Parse the cleaned novel. This can take a bit.'''\n",
    "    nlp = spacy.load('en')\n",
    "    text_doc = nlp(text)\n",
    "    print('nlp parsing done.')\n",
    "    text_sents = [[sent, label] for sent in text_doc.sents]\n",
    "    print('Sentences parsed.')\n",
    "    return text_doc, text_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Time to bag some words!  Since spaCy has already tokenized and labelled our data, we can move directly to recording how often various words occur.  We will exclude stopwords and punctuation.  In addition, in an attempt to keep our feature space from exploding, we will work with lemmas (root words) rather than the raw text terms, and we'll only use the 2000 most common words for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_WORD_COUNT = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(COMMON_WORD_COUNT)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 100 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: turn most common words into features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_common_word_features_df(text1_doc, tex2_doc, text1_sents, text2_sents):\n",
    "    common_words = set(bag_of_words(text1_doc) + bag_of_words(tex2_doc))\n",
    "    sentences = pd.DataFrame(text1_sents + text2_sents)\n",
    "    return bow_features(sentences, common_words), common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run BoW on *Alice* and *Persuasion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp parsing done.\n",
      "Sentences parsed.\n",
      "nlp parsing done.\n",
      "Sentences parsed.\n"
     ]
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "alice_doc, alice_sents = parse_into_doc_and_sents(load_and_clean_text('carroll-alice.txt', r'CHAPTER .*'), 'Caroll')\n",
    "persuasion_doc, persuasion_sents = parse_into_doc_and_sents(load_and_clean_text('austen-persuasion.txt', r'Chapter \\d+'), 'Austen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 100\n",
      "Processing row 200\n",
      "Processing row 300\n",
      "Processing row 400\n",
      "Processing row 500\n",
      "Processing row 600\n",
      "Processing row 700\n",
      "Processing row 800\n",
      "Processing row 900\n",
      "Processing row 1000\n",
      "Processing row 1100\n",
      "Processing row 1200\n",
      "Processing row 1300\n",
      "Processing row 1400\n",
      "Processing row 1500\n",
      "Processing row 1600\n",
      "Processing row 1700\n",
      "Processing row 1800\n",
      "Processing row 1900\n",
      "Processing row 2000\n",
      "Processing row 2100\n",
      "Processing row 2200\n",
      "Processing row 2300\n",
      "Processing row 2400\n",
      "Processing row 2500\n",
      "Processing row 2600\n",
      "Processing row 2700\n",
      "Processing row 2800\n",
      "Processing row 2900\n",
      "Processing row 3000\n",
      "Processing row 3100\n",
      "Processing row 3200\n",
      "Processing row 3300\n",
      "Processing row 3400\n",
      "Processing row 3500\n",
      "Processing row 3600\n",
      "Processing row 3700\n",
      "Processing row 3800\n",
      "Processing row 3900\n",
      "Processing row 4000\n",
      "Processing row 4100\n",
      "Processing row 4200\n",
      "Processing row 4300\n",
      "Processing row 4400\n",
      "Processing row 4500\n",
      "Processing row 4600\n",
      "Processing row 4700\n",
      "Processing row 4800\n",
      "Processing row 4900\n",
      "Processing row 5000\n",
      "Processing row 5100\n",
      "Processing row 5200\n",
      "Processing row 5300\n"
     ]
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts, common_words = create_common_word_features_df(alice_doc, persuasion_doc, alice_sents, persuasion_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_and_y(word_counts):\n",
    "    Y = word_counts['text_source']\n",
    "    X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance: Austen comprises 68.62% of dataset.\n",
      "\n",
      "Cross Val (5 folds):\n",
      "Avg: 89.11 \n",
      "Std: 1.9\n",
      "Training set score: 0.9200626959247649\n",
      "\n",
      "Test set score: 0.9041353383458647\n",
      "\n",
      "Confusion matrix for test set:\n",
      "text_source  Austen  Caroll\n",
      "row_0                      \n",
      "Austen         1428     160\n",
      "Caroll           44     496\n",
      "Sensitivity, hit rate, recall, or true positive rate:  0.7560975609756098\n",
      "Specificity or true negative rate:  0.970108695652174\n",
      "Precision or positive predictive value:  0.9185185185185185\n",
      "Negative predictive value:  0.8992443324937027\n",
      "Fall out or false positive rate:  0.029891304347826088\n",
      "False negative rate:  0.24390243902439024\n",
      "False discovery rate:  [0.10075567 0.08148148]\n",
      "Overall accuracy:  [0.90413534 0.90413534]\n",
      "None\n",
      "__DONE__\n"
     ]
    }
   ],
   "source": [
    "X, y = get_x_and_y(word_counts)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "alice_persuasion_lr, X, y = run_model(lr, X=X, y=y, folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## RF\n",
    "\n",
    "Now let's give the bag of words features a whirl by trying a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance: Austen comprises 68.62% of dataset.\n",
      "\n",
      "Cross Val (5 folds):\n",
      "Avg: 86.63 \n",
      "Std: 2.56\n",
      "Training set score: 0.9658307210031348\n",
      "\n",
      "Test set score: 0.8778195488721805\n",
      "\n",
      "Confusion matrix for test set:\n",
      "text_source  Austen  Caroll\n",
      "row_0                      \n",
      "Austen         1375     163\n",
      "Caroll           97     493\n",
      "Sensitivity, hit rate, recall, or true positive rate:  0.7515243902439024\n",
      "Specificity or true negative rate:  0.9341032608695652\n",
      "Precision or positive predictive value:  0.8355932203389831\n",
      "Negative predictive value:  0.8940182054616385\n",
      "Fall out or false positive rate:  0.06589673913043478\n",
      "False negative rate:  0.24847560975609756\n",
      "False discovery rate:  [0.10598179 0.16440678]\n",
      "Overall accuracy:  [0.87781955 0.87781955]\n",
      "None\n",
      "__DONE__\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=10)\n",
    "_rfc = run_model(rfc, X=X, y=y, folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Logistic regression performs a bit better than the random forest.  \n",
    "\n",
    "# BoW with Gradient Boosting\n",
    "\n",
    "And finally, let's see what gradient boosting can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance: Austen comprises 68.62% of dataset.\n",
      "\n",
      "Cross Val (5 folds):\n",
      "Avg: 85.78 \n",
      "Std: 2.79\n",
      "Training set score: 0.8840125391849529\n",
      "\n",
      "Test set score: 0.8740601503759399\n",
      "\n",
      "Confusion matrix for test set:\n",
      "text_source  Austen  Caroll\n",
      "row_0                      \n",
      "Austen         1444     240\n",
      "Caroll           28     416\n",
      "Sensitivity, hit rate, recall, or true positive rate:  0.6341463414634146\n",
      "Specificity or true negative rate:  0.9809782608695652\n",
      "Precision or positive predictive value:  0.9369369369369369\n",
      "Negative predictive value:  0.8574821852731591\n",
      "Fall out or false positive rate:  0.019021739130434784\n",
      "False negative rate:  0.36585365853658536\n",
      "False discovery rate:  [0.14251781 0.06306306]\n",
      "Overall accuracy:  [0.87406015 0.87406015]\n",
      "None\n",
      "__DONE__\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "               learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "               n_iter_no_change=None, presort='auto', random_state=None,\n",
       "               subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "               verbose=0, warm_start=False), array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 1, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=object), 0       Caroll\n",
       " 1       Caroll\n",
       " 2       Caroll\n",
       " 3       Caroll\n",
       " 4       Caroll\n",
       " 5       Caroll\n",
       " 6       Caroll\n",
       " 7       Caroll\n",
       " 8       Caroll\n",
       " 9       Caroll\n",
       " 10      Caroll\n",
       " 11      Caroll\n",
       " 12      Caroll\n",
       " 13      Caroll\n",
       " 14      Caroll\n",
       " 15      Caroll\n",
       " 16      Caroll\n",
       " 17      Caroll\n",
       " 18      Caroll\n",
       " 19      Caroll\n",
       " 20      Caroll\n",
       " 21      Caroll\n",
       " 22      Caroll\n",
       " 23      Caroll\n",
       " 24      Caroll\n",
       " 25      Caroll\n",
       " 26      Caroll\n",
       " 27      Caroll\n",
       " 28      Caroll\n",
       " 29      Caroll\n",
       "          ...  \n",
       " 5288    Austen\n",
       " 5289    Austen\n",
       " 5290    Austen\n",
       " 5291    Austen\n",
       " 5292    Austen\n",
       " 5293    Austen\n",
       " 5294    Austen\n",
       " 5295    Austen\n",
       " 5296    Austen\n",
       " 5297    Austen\n",
       " 5298    Austen\n",
       " 5299    Austen\n",
       " 5300    Austen\n",
       " 5301    Austen\n",
       " 5302    Austen\n",
       " 5303    Austen\n",
       " 5304    Austen\n",
       " 5305    Austen\n",
       " 5306    Austen\n",
       " 5307    Austen\n",
       " 5308    Austen\n",
       " 5309    Austen\n",
       " 5310    Austen\n",
       " 5311    Austen\n",
       " 5312    Austen\n",
       " 5313    Austen\n",
       " 5314    Austen\n",
       " 5315    Austen\n",
       " 5316    Austen\n",
       " 5317    Austen\n",
       " Name: text_source, Length: 5318, dtype: object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "run_model(clf, X=X, y=y, folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Looks like logistic regression is the winner, but there's room for improvement.\n",
    "\n",
    "# Same model, new inputs\n",
    "\n",
    "What if we feed the model a different novel by Jane Austen, like _Emma_?  Will it be able to distinguish Austen from Carroll with the same level of accuracy if we insert a different sample of Austen's writing?\n",
    "\n",
    "First, we need to process _Emma_ the same way we processed the other data, and combine it with the Alice data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Emma on Alice-Persuasion trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp parsing done.\n",
      "Sentences parsed.\n",
      "Processing row 0\n",
      "Processing row 100\n",
      "Processing row 200\n",
      "Processing row 300\n",
      "Processing row 400\n",
      "Processing row 500\n",
      "Processing row 600\n",
      "Processing row 700\n",
      "Processing row 800\n",
      "Processing row 900\n",
      "Processing row 1000\n",
      "Processing row 1100\n",
      "Processing row 1200\n",
      "Processing row 1300\n",
      "Processing row 1400\n",
      "Processing row 1500\n",
      "Processing row 1600\n"
     ]
    }
   ],
   "source": [
    "emma_regex = r'(VOLUME \\w+)|(CHAPTER \\w+)'\n",
    "emma_doc, emma_sents = parse_into_doc_and_sents(load_and_clean_text('austen-emma.txt', emma_regex), 'Austen')\n",
    "# Build a new Bag of Words data frame for Emma word counts.\n",
    "# We'll use the same common words from Alice and Persuasion.\n",
    "emma_sentences = pd.DataFrame(emma_sents[0:len(alice_sents)])\n",
    "emma_bow = bow_features(emma_sentences, common_words)\n",
    "\n",
    "# Combine the Emma sentence data with ONLY the Alice data (exclude Persuasion)\n",
    "X_Emma_Alice = np.concatenate((\n",
    "    X[y[y == 'Caroll'].index],\n",
    "    emma_bow.drop(['text_sentence','text_source'], 1)\n",
    "), axis=0)\n",
    "\n",
    "y_Emma_Alice = pd.concat([y[y == 'Caroll'],\n",
    "                         pd.Series(['Austen'] * emma_bow.shape[0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance: Austen comprises 50.0% of dataset.\n",
      "Training set score: 0.8661338661338661\n",
      "\n",
      "Test set score: 0.8390718562874252\n",
      "\n",
      "Confusion matrix for test set:\n",
      "col_0   Austen  Caroll\n",
      "row_0                 \n",
      "Austen     588     156\n",
      "Caroll      59     533\n",
      "Sensitivity, hit rate, recall, or true positive rate:  0.7735849056603774\n",
      "Specificity or true negative rate:  0.9088098918083463\n",
      "Precision or positive predictive value:  0.9003378378378378\n",
      "Negative predictive value:  0.7903225806451613\n",
      "Fall out or false positive rate:  0.09119010819165378\n",
      "False negative rate:  0.22641509433962265\n",
      "False discovery rate:  [0.20967742 0.09966216]\n",
      "Overall accuracy:  [0.83907186 0.83907186]\n",
      "None\n",
      "__DONE__\n"
     ]
    }
   ],
   "source": [
    "emma_alice_lr, _X, _y = run_model(alice_persuasion_lr, X=X_Emma_Alice, y=y_Emma_Alice, pretrained_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Well look at that!  NLP approaches are generally effective on the same type of material as they were trained on. It looks like this model is actually able to differentiate multiple works by Austen from Alice in Wonderland.  Now the question is whether the model is very good at identifying Austen, or very good at identifying Alice in Wonderland, or both...\n",
    "\n",
    "# Challenge 0:\n",
    "\n",
    "Recall that the logistic regression model's best performance on the test set was 93%.  See what you can do to improve performance.  Suggested avenues of investigation include: Other modeling techniques (SVM?), making more features that take advantage of the spaCy information (include grammar, phrases, POS, etc), making sentence-level features (number of words, amount of punctuation), or including contextual information (length of previous and next sentences, words repeated from one sentence to the next, etc), and anything else your heart desires.  Make sure to design your models on the test set, or use cross_validation with multiple folds, and see if you can get accuracy above 90%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance: Austen comprises 68.62% of dataset.\n",
      "\n",
      "Cross Val (5 folds):\n",
      "Avg: 85.77 \n",
      "Std: 2.2\n",
      "Training set score: 0.8755485893416928\n",
      "\n",
      "Test set score: 0.8665413533834586\n",
      "\n",
      "Confusion matrix for test set:\n",
      "text_source  Austen  Caroll\n",
      "row_0                      \n",
      "Austen         1449     261\n",
      "Caroll           23     395\n",
      "Sensitivity, hit rate, recall, or true positive rate:  0.6021341463414634\n",
      "Specificity or true negative rate:  0.984375\n",
      "Precision or positive predictive value:  0.9449760765550239\n",
      "Negative predictive value:  0.8473684210526315\n",
      "Fall out or false positive rate:  0.015625\n",
      "False negative rate:  0.3978658536585366\n",
      "False discovery rate:  [0.15263158 0.05502392]\n",
      "Overall accuracy:  [0.86654135 0.86654135]\n",
      "None\n",
      "__DONE__\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm_classifier = svm.SVC(gamma='scale')\n",
    "alice_persuasion_svm = run_model(svm_classifier, X=X, y=y, folds=5, pretrained_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Challenge 1:\n",
    "Find out whether your new model is good at identifying Alice in Wonderland vs any other work, Persuasion vs any other work, or Austen vs any other work.  This will involve pulling a new book from the Project Gutenberg corpus (print(gutenberg.fileids()) for a list) and processing it.\n",
    "\n",
    "Record your work for each challenge in a notebook and submit it below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prep Bible text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp parsing done.\n",
      "Sentences parsed.\n"
     ]
    }
   ],
   "source": [
    "bible_doc, bible_sents = parse_into_doc_and_sents(load_and_clean_text('bible-kjv.txt', r'\\n?\\d+\\:\\d+'), 'Bible')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train new model on Bible and Persuasion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 100\n",
      "Processing row 200\n",
      "Processing row 300\n",
      "Processing row 400\n",
      "Processing row 500\n",
      "Processing row 600\n",
      "Processing row 700\n",
      "Processing row 800\n",
      "Processing row 900\n",
      "Processing row 1000\n",
      "Processing row 1100\n",
      "Processing row 1200\n",
      "Processing row 1300\n",
      "Processing row 1400\n",
      "Processing row 1500\n",
      "Processing row 1600\n",
      "Processing row 1700\n",
      "Processing row 1800\n",
      "Processing row 1900\n",
      "Processing row 2000\n",
      "Processing row 2100\n",
      "Processing row 2200\n",
      "Processing row 2300\n",
      "Processing row 2400\n",
      "Processing row 2500\n",
      "Processing row 2600\n",
      "Processing row 2700\n",
      "Processing row 2800\n",
      "Processing row 2900\n",
      "Processing row 3000\n",
      "Processing row 3100\n",
      "Processing row 3200\n",
      "Processing row 3300\n",
      "Processing row 3400\n",
      "Processing row 3500\n",
      "Processing row 3600\n",
      "Processing row 3700\n",
      "Processing row 3800\n",
      "Processing row 3900\n",
      "Processing row 4000\n",
      "Processing row 4100\n",
      "Processing row 4200\n",
      "Processing row 4300\n",
      "Processing row 4400\n",
      "Processing row 4500\n",
      "Processing row 4600\n",
      "Processing row 4700\n",
      "Processing row 4800\n",
      "Processing row 4900\n",
      "Processing row 5000\n",
      "Processing row 5100\n",
      "Processing row 5200\n",
      "Processing row 5300\n",
      "Processing row 5400\n",
      "Processing row 5500\n",
      "Processing row 5600\n",
      "Processing row 5700\n",
      "Processing row 5800\n",
      "Processing row 5900\n",
      "Processing row 6000\n",
      "Processing row 6100\n",
      "Processing row 6200\n",
      "Processing row 6300\n",
      "Processing row 6400\n",
      "Processing row 6500\n",
      "Processing row 6600\n",
      "Processing row 6700\n",
      "Processing row 6800\n",
      "Processing row 6900\n",
      "Processing row 7000\n",
      "Processing row 7100\n",
      "Processing row 7200\n",
      "Processing row 7300\n",
      "Processing row 7400\n",
      "Processing row 7500\n",
      "Processing row 7600\n",
      "Processing row 7700\n",
      "Processing row 7800\n",
      "Processing row 7900\n",
      "Processing row 8000\n",
      "Processing row 8100\n",
      "Processing row 8200\n",
      "Processing row 8300\n",
      "Processing row 8400\n",
      "Processing row 8500\n",
      "Processing row 8600\n",
      "Processing row 8700\n",
      "Processing row 8800\n",
      "Processing row 8900\n",
      "Processing row 9000\n",
      "Processing row 9100\n",
      "Processing row 9200\n",
      "Processing row 9300\n",
      "Processing row 9400\n",
      "Processing row 9500\n",
      "Processing row 9600\n",
      "Processing row 9700\n",
      "Processing row 9800\n",
      "Processing row 9900\n",
      "Processing row 10000\n",
      "Processing row 10100\n",
      "Processing row 10200\n",
      "Processing row 10300\n",
      "Processing row 10400\n"
     ]
    }
   ],
   "source": [
    "bible_persuasion_df, bible_persuasion_common_words = create_common_word_features_df(persuasion_doc, bible_doc, persuasion_sents, bible_sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance: Bible comprises 65.17% of dataset.\n",
      "\n",
      "Cross Val (5 folds):\n",
      "Avg: 96.97 \n",
      "Std: 0.51\n",
      "Training set score: 0.9789976133651551\n",
      "\n",
      "Test set score: 0.9677881173944166\n",
      "\n",
      "Confusion matrix for test set:\n",
      "text_source  Austen  Bible\n",
      "row_0                     \n",
      "Austen         1382    109\n",
      "Bible            26   2674\n",
      "Sensitivity, hit rate, recall, or true positive rate:  0.9608336327703917\n",
      "Specificity or true negative rate:  0.9815340909090909\n",
      "Precision or positive predictive value:  0.9903703703703703\n",
      "Negative predictive value:  0.9268947015425889\n",
      "Fall out or false positive rate:  0.018465909090909092\n",
      "False negative rate:  0.03916636722960833\n",
      "False discovery rate:  [0.0731053  0.00962963]\n",
      "Overall accuracy:  [0.96778812 0.96778812]\n",
      "None\n",
      "__DONE__\n"
     ]
    }
   ],
   "source": [
    "X_bible_persuasion, y_bible_persuasion = get_x_and_y(bible_persuasion_df)\n",
    "bible_persuasion_lr, _X, _y = run_model(lr, X=X_bible_persuasion, y=y_bible_persuasion, pretrained_model=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST Bible-Persusasion model with Bible-Emma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 100\n",
      "Processing row 200\n",
      "Processing row 300\n",
      "Processing row 400\n",
      "Processing row 500\n",
      "Processing row 600\n",
      "Processing row 700\n",
      "Processing row 800\n",
      "Processing row 900\n",
      "Processing row 1000\n",
      "Processing row 1100\n",
      "Processing row 1200\n",
      "Processing row 1300\n",
      "Processing row 1400\n",
      "Processing row 1500\n",
      "Processing row 1600\n",
      "Processing row 1700\n",
      "Processing row 1800\n",
      "Processing row 1900\n",
      "Processing row 2000\n",
      "Processing row 2100\n",
      "Processing row 2200\n",
      "Processing row 2300\n",
      "Processing row 2400\n",
      "Processing row 2500\n",
      "Processing row 2600\n",
      "Processing row 2700\n",
      "Processing row 2800\n",
      "Processing row 2900\n",
      "Processing row 3000\n",
      "Processing row 3100\n",
      "Processing row 3200\n",
      "Processing row 3300\n",
      "Processing row 3400\n",
      "Processing row 3500\n",
      "Processing row 3600\n",
      "Processing row 3700\n",
      "Processing row 3800\n",
      "Processing row 3900\n",
      "Processing row 4000\n",
      "Processing row 4100\n",
      "Processing row 4200\n",
      "Processing row 4300\n",
      "Processing row 4400\n",
      "Processing row 4500\n",
      "Processing row 4600\n",
      "Processing row 4700\n",
      "Processing row 4800\n",
      "Processing row 4900\n",
      "Processing row 5000\n",
      "Processing row 5100\n",
      "Processing row 5200\n",
      "Processing row 5300\n",
      "Processing row 5400\n",
      "Processing row 5500\n",
      "Processing row 5600\n",
      "Processing row 5700\n",
      "Processing row 5800\n",
      "Processing row 5900\n",
      "Processing row 6000\n",
      "Processing row 6100\n",
      "Processing row 6200\n",
      "Processing row 6300\n",
      "Processing row 6400\n",
      "Processing row 6500\n",
      "Processing row 6600\n",
      "Processing row 6700\n",
      "Processing row 6800\n"
     ]
    }
   ],
   "source": [
    "# Build a new Bag of Words data frame for Emma word counts.\n",
    "# We'll use the same common words from Bible and Persuasion.\n",
    "\n",
    "emma_sentences = pd.DataFrame(emma_sents[0:len(bible_sents)])  # Hack in case emma is shorter\n",
    "emma_bow = bow_features(emma_sentences, bible_persuasion_common_words)\n",
    "\n",
    "emma_x_to_concat = emma_bow.drop(['text_sentence','text_source'], 1)\n",
    "bible_x_to_concat = X_bible_persuasion[y_bible_persuasion[y_bible_persuasion == 'Bible'].index]\n",
    "\n",
    "\n",
    "\n",
    "# Combine the Emma sentence data with ONLY the Bible data (exclude Persuasion)\n",
    "X_Emma_Bible = np.concatenate((\n",
    "    bible_x_to_concat,\n",
    "    emma_x_to_concat\n",
    "), axis=0)\n",
    "\n",
    "y_Emma_Bible = pd.concat([y_bible_persuasion[y_bible_persuasion == 'Bible'],\n",
    "                         pd.Series(['Austen'] * emma_bow.shape[0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance: Austen comprises 50.0% of dataset.\n",
      "Training set score: 0.9591064453125\n",
      "\n",
      "Test set score: 0.9635664591724643\n",
      "\n",
      "Confusion matrix for test set:\n",
      "col_0   Austen  Bible\n",
      "row_0                \n",
      "Austen    2665     73\n",
      "Bible      126   2598\n",
      "Sensitivity, hit rate, recall, or true positive rate:  0.9726694122051666\n",
      "Specificity or true negative rate:  0.9548548907201719\n",
      "Precision or positive predictive value:  0.9537444933920705\n",
      "Negative predictive value:  0.9733382030679328\n",
      "Fall out or false positive rate:  0.04514510927982802\n",
      "False negative rate:  0.027330587794833397\n",
      "False discovery rate:  [0.0266618  0.04625551]\n",
      "Overall accuracy:  [0.96356646 0.96356646]\n",
      "None\n",
      "__DONE__\n"
     ]
    }
   ],
   "source": [
    "bible_emma_lr, _X, _y = run_model(bible_persuasion_lr, X=X_Emma_Bible, y=y_Emma_Bible, pretrained_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval\n",
    "## Model trained on Bible-Persuasion still works well on Bible-Emma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
